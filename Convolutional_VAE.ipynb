{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutional VAE.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPF5TaqxVouoJsFQmXt/5Jo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/makkimaki/VAEprojects/blob/master/Convolutional_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4STqpio_S9a_",
        "outputId": "b46425a2-838d-4bff-dd9c-e19cc758d00e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import norm\n",
        "\n",
        "import glob\n",
        "import imageio\n",
        "import PIL\n",
        "from IPython import display\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import metrics\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow_probability as tfp\n",
        "import time\n",
        "\n",
        "np.random.seed(42)\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAGrnmlLUR3N",
        "outputId": "9f1c1210-0569-4b48-afd8-f1e65e26fb31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-bG1P5XhLJ0",
        "outputId": "88edd4e4-5c3d-4737-9eaa-e8294251a47e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FEEBMj1U0uk"
      },
      "source": [
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > 0.5, 1.0, 0.0).astype(\"float32\")\n",
        "\n",
        "train_images_preprocessed = preprocess_images(train_images)\n",
        "test_images_preprocessed = preprocess_images(test_images)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXLhuqSChNl5",
        "outputId": "e0490469-b002-4a7b-accd-21b3db27f57c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_images_preprocessed.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4BnSFoLg421",
        "outputId": "bf8381cb-7a53-4edc-d978-4990fc541b3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "plt.figure(1)\n",
        "plt.subplot(221)\n",
        "plt.imshow(train_images[0, :, :])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"row image\")\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(train_images_preprocessed[0, :, :, 0])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"preprocessed image\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'preprocessed image')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_U_1lXbhTNg"
      },
      "source": [
        "train_size = 60000\n",
        "batch_size = 32\n",
        "test_size = 10000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQiIMG5ziczm"
      },
      "source": [
        "## Use tf.data to batch and shuffle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAGkkn1MibUg"
      },
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images_preprocessed).shuffle(train_size).batch(batch_size))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images_preprocessed).shuffle(test_size).batch(batch_size))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaYsiCtxit6z",
        "outputId": "5d62534d-2e1b-48c0-a846-7f1151689eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: (None, 28, 28, 1), types: tf.float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glWk78mmkhLp"
      },
      "source": [
        "class CVAE(tf.keras.Model):\n",
        "  \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_dim):\n",
        "    super(CVAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.InputLayer(input_shape=(28, 28, 1)),\n",
        "         tf.keras.layers.Conv2D(\n",
        "             filters=32, kernel_size=3, strides=(2,2), activation=\"relu\"),\n",
        "         tf.keras.layers.Conv2D(\n",
        "             filters=64, kernel_size=3, strides=(2,2), activation=\"relu\"),\n",
        "         tf.keras.layers.Flatten(),\n",
        "         # no activation\n",
        "         tf.keras.layeres.Dense(latent_dim + latend_dim),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "         tf.keras.layers.InputLayer(input_shape=(latent_dim,)),\n",
        "         tf.keras.layers.Dense(units=7*7*32, activation=\"relu\"),\n",
        "         tf.keras.layers.Reshape(target_shape=(7,7,32)),\n",
        "         tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding=\"same\", activation=\"relu\"),\n",
        "         tf.keras.layers.Conv2DTranspose(filters=1, kernel_size=3, strides=1, padding=\"same\")\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    @tf.function\n",
        "    def sample(self, eps=None):\n",
        "      if eps is None:\n",
        "        eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "      return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "    def encode(self, x):\n",
        "      mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)\n",
        "      return mean, logvar\n",
        "\n",
        "    def reparameterize(self, mean, logvar):\n",
        "      eps = tf.random.normal(shape=mean.shape)\n",
        "      return eps * tf.exp(logvar * 0.5) + mean\n",
        "\n",
        "    def decode(self, z, apply_sigmoid=False):\n",
        "      logits = self.decoder(z)\n",
        "      if apply_sigmoid:\n",
        "        probs = tf.sigmoid(logits)\n",
        "        return probs\n",
        "      return logits"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjCuG3_dvQ89"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  log2pi = tf.math.log(2 * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -0.5 * ((sample - mean)**2 * tf.exp(-logvar) + logvar + log2pi), axis= raxis \n",
        "  )\n",
        "\n",
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2,3])\n",
        "  logpz_x = log_normal_pdf(z, 0, 0)\n",
        "  logqz_x = log_normal_pdf(z, mean, logvar)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "  \"\"\"Executes one training step and returns the loss.\n",
        "\n",
        "  This function computes the loss and gradients, and uses the latter to update the model's parameters.\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSj0fDEwvoM1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}